# Staging tables are temporary tables that can be used to perform data cleansing, data transformations, and data validation. 
You can also use staging tables to load data from multiple sources into a single destination table.

# Generally, you should implement a data warehouse load process that performs tasks in the following order:
1. Ingest the new data to be loaded into a data lake, applying pre-load cleansing or transformations as required.
2. Load the data from files into staging tables in the relational data warehouse.
3. Load the dimension tables from the dimension data in the staging tables, updating existing rows or inserting new rows and generating surrogate key values as necessary.
4. Load the fact tables from the fact data in the staging tables, looking up the appropriate surrogate keys for related dimensions.
5. Perform post-load optimization by updating indexes and table distribution statistics.

# Measures are calculated columns that are based on the data in the tables in your data warehouse using the Data Analysis Expressions (DAX) formula language.

# Relationships allow you to connect tables in the semantic model. Create relationships between tables in your data warehouse using a click-and-drag interface in Fabric in the Model view.
Cardinality helps to maintain One-One, One-Many, Many-One, Many-Many relationships.

# Every time a data warehouse is created, Fabric creates a semantic model (which consists of metrics) for analysts and/or business users to connect to for report creation.
New tables in the Lakehouse are automatically added to the default semantic model. 
Semantic models are automatically kept in sync with the data warehouse, so you don't have to worry about maintaining them. 
Hence the customic semantic models created based on specific business requirements would get updated automatcally.
Users can also manually select tables or views from the warehouse they want included in the model for more flexibility. 
Objects that are in the default semantic model are created as a layout in the model view.

# When you display a dataframe or run a SQL query in a Spark notebook (Spark SQL), the results are displayed under the code cell can be viewed in Table format or Chart format (Data Visualization). 
By default, results are rendered as a table, but you can also change the results view to a chart and use the chart properties to customize how the chart visualizes the data, as shown here:
%%SQL
Select * From Patients
This code could generate Pie Chart Visualization on Spark Notebook.

# Datamarts are recommended for customers who need domain oriented, decentralized data ownership and architecture, 
such as users who need data as a product or a self-service data platform.
Datamarts are a fully managed database that enables you to store and explore your data in a relational and fully managed Azure SQL DB.
Incremental refresh extends scheduled refresh operations by providing automated partition creation and management for datamart tables that frequently load new and updated data.
For most datamarts, incremental refresh will involve one or more tables that contain transaction data that changes often and can grow exponentially, such as a fact table in a relational or star database schema. 
If you use an incremental refresh policy to partition the table, and refresh only the most recent import partitions, you can significantly reduce the amount of data that must be refreshed.

https://learn.microsoft.com/en-us/power-bi/transform-model/datamarts/datamarts-overview
